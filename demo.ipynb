{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0557c4",
   "metadata": {},
   "source": [
    "# Orthogonal Matching Pursuit demostration\n",
    "\n",
    "We start by importing some necessary standard libraries, the Orthogonal Matching Pursuit algorithms (DCT and Gabor dictionaries implementation) and some helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79471e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from omp import OMP_C, OMP_G\n",
    "from helpers import next_multiple, define_reliable_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba44199",
   "metadata": {},
   "source": [
    "In order to tackle all the preprocessing steps needed before calling the algorithm, namely: the creation of the dictionaries, the frame-based processing and the overlap-add method an utilitary function was built. Its lines are accompanied by comments that explain what is being done in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6177c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_OMP(file, audio, fs, dictionary, mode, verbose):\n",
    "    '''\n",
    "    invoke_OMP\n",
    "    Utilitary function to call the Orthogonal Matching Pursuit (OMP) algorithm and return the audio signal estimation.\n",
    "    \n",
    "    Parameters\n",
    "    file: File name.\n",
    "    y: Signal to estimate.\n",
    "    fs: Sample rate of the signal.\n",
    "    D: Toggles the DCT or DGT dictionaries. Takes 'D' or 'G' as values, respectively.\n",
    "    mode: Whether to enforce the unconstrained ('normal') or the minimum constrained ('min') problem.\n",
    "    verbose: Toggles verbosity.\n",
    "    '''\n",
    "    \n",
    "    # The chosen frame length is 64 ms as defined by manual setting in the paper. \n",
    "    # Thus, we multiply the sample rate times 64 ms in order to obtain the number of samples per frame.\n",
    "    N = int(fs * 64e-3)\n",
    "    N_support = np.arange(N)\n",
    "\n",
    "    # In order to make frame-based processing possible we need to pad with zeros the original audio vector. \n",
    "    # Thereforex, we need to change its length.\n",
    "    L = next_multiple(len(audio), N)\n",
    "    audio = np.pad(audio, (0, L - len(audio)))\n",
    "\n",
    "    # The clipping level is obtained from considering the infinity norm of the audio vector. \n",
    "    CLIP_LEVEL = max(audio)\n",
    "    \n",
    "    # We assign a different value to the frequency bins K variable depending on what dictionary we are working on.\n",
    "    if dictionary =='C':\n",
    "        K = N * 2\n",
    "        K_support = np.arange(K)\n",
    "\n",
    "    elif dictionary =='G':\n",
    "        K = N\n",
    "        K_support = np.arange(K)\n",
    "\n",
    "    ''' Dictionary '''\n",
    "    \n",
    "    # We will use a rectangular window for the dictionary and as the analysis window per frame.\n",
    "    w = np.ones(N)\n",
    "\n",
    "    # DCT.\n",
    "    if dictionary =='C':\n",
    "        # Memory allocation for the dictionary.\n",
    "        D = np.empty((N, K))\n",
    "        \n",
    "        # Now we gradually complete each column of the dictionary through iteration for each case.\n",
    "        for j in K_support:\n",
    "            D[:, j] = w * np.cos((np.pi / K) * (N_support + 1/2) * (j + 1/2))\n",
    "\n",
    "    # Gabor.\n",
    "    elif dictionary =='G':\n",
    "        D = np.empty((N, 2 * K))\n",
    "        \n",
    "        for j in K_support:\n",
    "            D[:, j] = w * np.cos((np.pi / K) * (N_support + 1/2) * (j + 1/2))\n",
    "            D[:, j + K] = w * np.sin((np.pi / K) * (N_support + 1/2) * (j + 1/2))\n",
    "\n",
    "    ''' Frame-based processing '''\n",
    "\n",
    "    # In order to achieve 75% overlap, the offset distance (hop size) must be of N / 4.\n",
    "    R = N // 4\n",
    "    R_support = np.arange(0, L - N + 1, R)\n",
    "\n",
    "    # The total number of frames is obtained.\n",
    "    frames = len(R_support)\n",
    "\n",
    "    # The memory allocation for the matrix that will contain each of the frames is performed.\n",
    "    Y = np.empty((N, frames))\n",
    "\n",
    "    # The rectangular window is applied and each frame is assigned to the matrix Y.\n",
    "    for i, r in enumerate(R_support):\n",
    "        Y[:, i] = w * audio[r:r+N]\n",
    "\n",
    "    # We will also generate the measurement matrix M from an identity matrix of dimension N x N.\n",
    "    M = np.identity(N)\n",
    "\n",
    "    # Memory allocation for the matrix that will contain the processed frames.\n",
    "    # DCT.\n",
    "    if dictionary =='C':\n",
    "        X_k = np.empty((K, frames))\n",
    "\n",
    "    # Gabor.\n",
    "    elif dictionary =='G':\n",
    "        X_k = np.empty((2 * K, frames))\n",
    "\n",
    "    print('OMP - filename: {}, dictionary: {}, mode: {}'.format(file, dictionary, mode))\n",
    "    \n",
    "    # From here on, we will process each frame independently and put it in each column of the matrix X_k.\n",
    "    for i in np.arange(frames):\n",
    "            if verbose: print('\\nframe number: {}'.format(i))\n",
    "\n",
    "            # Frame assignment.\n",
    "            y = Y[:, i]\n",
    "\n",
    "            # The support of the reliable samples I_r. \n",
    "            # It is obtained through finding the samples where the clipping level is not reached.\n",
    "            I_r = define_reliable_support(y, CLIP_LEVEL)\n",
    "            \n",
    "            # The reliable samples support vector is used to obtain the reliable samples vector.\n",
    "            y_r = y[I_r]\n",
    "\n",
    "            # The support of the missing samples I_m.\n",
    "            # Conversely, It is obtained through finding the samples that result in the \n",
    "            # absolute value of the clipping level when evaluated in the signal.\n",
    "            I_mp = np.where(y == CLIP_LEVEL)[0]\n",
    "            I_mn = np.where(y == - CLIP_LEVEL)[0]\n",
    "            \n",
    "            # Measurement matrices are built upon the support vectors also.\n",
    "            M_r = M[I_r]\n",
    "            M_mp = M[I_mp]\n",
    "            M_mn = M[I_mn]\n",
    "\n",
    "            # The error threshold will be set to 1e-6. \n",
    "            epsilon = 1e-6\n",
    "\n",
    "            # Further, we multiply this value for the amount of elements in the reliable samples support vector.\n",
    "            epsilon_omp = epsilon * len(I_r)\n",
    "\n",
    "            # The maximum sparsity level is defined. \n",
    "            # Floor division is used to keep the result as an integer and make comparison operators work more efficiently.\n",
    "            K_omp = N // 4\n",
    "\n",
    "            # The OMP algorithm is called.\n",
    "            if dictionary =='C':\n",
    "                x_k = OMP_C(y_r, M_r, M_mp, M_mn, D, K_omp, epsilon_omp, CLIP_LEVEL, mode=mode, verbose=verbose)\n",
    "            elif dictionary =='G':\n",
    "                x_k = OMP_G(y_r, I_r, M_r, M_mp, M_mn, D, K_omp, epsilon_omp, CLIP_LEVEL, mode=mode, verbose=verbose)\n",
    "\n",
    "            # After execution, we store the sparse representation into the previously allocated in memory matrix X_k.\n",
    "            X_k[:, i] = x_k\n",
    "\n",
    "    ''' Overlap-Add Method '''\n",
    "    \n",
    "    # Memory allocation of a list that will contain the processed signal and another that will contain a sum of windows.\n",
    "    proc_audio = np.zeros(L)\n",
    "    proc_w_sin = np.zeros(L)\n",
    "    \n",
    "    # A sine window is used as the synthesis window.\n",
    "    w_sin = sp.signal.windows.cosine(N)\n",
    "\n",
    "    # The signal is restored using the overlap-add method by definition.\n",
    "    for x_k, r in zip(X_k.T, R_support):\n",
    "        proc_audio[r:r+N] = proc_audio[r:r+N] + D.dot(x_k) * w_sin\n",
    "        proc_w_sin[r:r+N] = proc_w_sin[r:r+N] + w_sin\n",
    "\n",
    "    proc_audio = proc_audio / proc_w_sin\n",
    "\n",
    "    return proc_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f889c",
   "metadata": {},
   "source": [
    "As the above function only takes one signal at the time, a small script was written in order to load all audio signals and execute the declipping algorithms in a gradual fashion. Keep in mind that you need the signal directory in order to execute the next cell and that it takes about 5 minutes per second of signal estimation (~25 minutes per signal) on an average computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ac698",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''' Signal processing '''\n",
    "\n",
    "# The needed variables are initialized.\n",
    "verbose = False\n",
    "path = './signal/clipped/'\n",
    "\n",
    "original_signals = []\n",
    "clipped_signals = []\n",
    "d_m = []\n",
    "\n",
    "# A list with audio file names is created.\n",
    "files = os.listdir(path)\n",
    "\n",
    "# We proceed to load our clipped audio signals.\n",
    "for file in sorted(files):\n",
    "    if file[-4:] == '.wav':\n",
    "            [audio, fs] = librosa.load(path + file, sr=None) \n",
    "            clipped_signals.append((file, audio, fs))\n",
    "\n",
    "# Tuples are used to store all possible parameter combinations.\n",
    "for dictionary in ['C', 'G']:\n",
    "    for mode in ['normal', 'min']:\n",
    "        d_m.append((dictionary, mode))\n",
    "   \n",
    "# Finally, we call the declipping algorithm through the invoke_OMP utilitary function and create an audio .wav file with the generated audio signal estimation.\n",
    "i = 0\n",
    "for signal in clipped_signals:\n",
    "    for (dictionary, mode) in d_m:\n",
    "        \n",
    "        file = signal[0]\n",
    "        audio = signal[1]\n",
    "        fs = signal[2]\n",
    "        \n",
    "        y = invoke_OMP(file=file, audio=audio, fs=fs, dictionary=dictionary, mode=mode, verbose=verbose)\n",
    "        file = file.replace('.wav','',1)\n",
    "        sf.write('de'+file+'_'+'OMP'+dictionary+'_'+mode+'.wav', y, fs)\n",
    "        i += 1\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f6fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%latex\n",
    "\n",
    "In order to prove which of the algorithms makes the best estimation in terms of quality, we compare them using the Signal-to-Noise ratio $\\mathbf{SNR}$. This parameter provides a good measurement of the global restoration of the signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879ff57",
   "metadata": {},
   "source": [
    "Four different algorithms were tested on several clipping levels of a musical signal at a sample rate of 16 kHz. The first two algorithms (namely, the 'OMP C normal' and 'OMP G normal' algorithms) use the sparse representation framework with the DCT and Gabor transform to solve the audio inpainting problem, where the degraded samples are considered missing and the estimated signal is the result of a least-squares projection. \n",
    "\n",
    "On the other hand, the 'OMP C min' and 'OMP G min' algorithms also use the same **dictionaries**, the matrices of elementary signals comprised by the transforms, but solve the audio declipping problem instead, where the information about the clipping level is added to the model in order to improve the estimation.\n",
    "\n",
    "The results are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR = np.load('snr.npy')\n",
    "\n",
    "# Plots\n",
    "\n",
    "x = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "legends = ['OMP C normal', 'OMP G normal', 'OMP C min', 'OMP G min']\n",
    "\n",
    "colors = ['silver', 'black', 'silver', 'black']\n",
    "linestyles = ['-', '-', '--', '--']\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.title('music02_16kHz.wav', fontsize=18)\n",
    "plt.xlabel('Clipping level', fontsize=16)\n",
    "plt.ylabel('$SNR$', fontsize=16)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "for i, col, lin in zip(np.arange(4), colors, linestyles):\n",
    "    plt.plot(x, SNR[:,i], linestyle=lin, color=col)\n",
    "plt.grid(b=True, which='major', linestyle='--', alpha=0.7)\n",
    "plt.legend(['OMP C normal', 'OMP G normal', 'OMP C min', 'OMP G min'], fontsize=12, loc='upper left')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b0514",
   "metadata": {},
   "source": [
    "All the values of the previous plot are referenced to the Signal-to-Noise ratio of the clipped signal for each clipping level, which constitutes the 0 dB level. At a first glance, we can clearly see that the Gabor dictionary performs better in both versions of the algorithms (inpainting and declipping). This is because the Gabor dictionary considers the phase component of the audio signals in the resulting sparse representations whereas the Discrete Cosine Transform combines a group of cosine functions with zero phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a784ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "\n",
    "Alternatively, we can also consider the Signal-to-Noise ratio of the missing or clipped samples $\\mathbf{SNR}_m$. This objective parameter depends solely on the result of the algorithm and is obtained from $\\mathbf{SNR}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb353ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR_m = np.load('snr_m.npy')\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.title('music02_16kHz.wav', fontsize=20)\n",
    "plt.xlabel('Clipping level', fontsize=16)\n",
    "plt.ylabel('$SNR_m$', fontsize=16)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "for i, col, lin in zip(np.arange(4), colors, linestyles):\n",
    "    plt.plot(x, SNR_m[:,i], linestyle=lin, color=col)\n",
    "plt.grid(b=True, which='major', linestyle='--', alpha=0.4)\n",
    "plt.legend(['OMP C normal', 'OMP G normal', 'OMP C min', 'OMP G min'], fontsize=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a55869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "\n",
    "Finally, this plot shows a correlation between the $\\mathbf{SNR_m}$ and the clipping level of the four algorithms that reflects the improvement on the resulting audio signal estimation as the clipping levels increments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f82e11",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "[1] Adler, A., Emiya, V., Jafari, M. G., Elad, M., Gribonval, R., & Plumbley, M. D. (2011, May). A constrained matching pursuit approach to audio declipping. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 329-332). IEEE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
